{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.webdriver import WebDriver as ChromeWebDriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import json\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import datetime as dt\n",
    "import re\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "chrome_service = ChromeService(\"C:\\\\Users\\\\VICTUS\\\\Downloads\\\\chromedriver-win64\\\\chromedriver-win64\\\\chromedriver.exe\")\n",
    "driver = ChromeWebDriver(service=chrome_service)\n",
    "\n",
    "# Malaysia login url\n",
    "# login_url=\"https://shopee.com.my/buyer/login\"\n",
    "\n",
    "# Taiwan login_url\n",
    "login_url = \"https://shopee.tw/buyer/login\"\n",
    "driver.get(login_url)\n",
    "\n",
    "\n",
    "# language_button = driver.find_element(By.XPATH, '//*[@id=\"modal\"]/div[1]/div[1]/div/div[3]/div[1]/button')\n",
    "# language_button.click()\n",
    "\n",
    "username_field = driver.find_element(By.NAME, 'loginKey')\n",
    "password_field = driver.find_element(By.NAME, 'password')\n",
    "\n",
    "username_field.send_keys(\"amit@zethic.com\")\n",
    "password_field.send_keys(\"Amit@1234#\")\n",
    "\n",
    "login_button = driver.find_element(By.XPATH, \"//*[@id='main']/div/div[2]/div/div/div/div[2]/form/div/div[2]/button\")\n",
    "login_button.click()    \n",
    "\n",
    "# time.sleep(10)\n",
    "\n",
    "csv_filename = \"store.csv\"\n",
    "json_filename = \"store.json\"\n",
    "\n",
    "timestamp_last_scrape = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://shopee.tw/product/9241878/21870358942', 'https://shopee.tw/product/9241878/12687797847', 'https://shopee.tw/product/9241878/17848374918', 'https://shopee.tw/product/9241878/22315898309', 'https://shopee.tw/product/9241878/11118826864', 'https://shopee.tw/product/9241878/12087756208', 'https://shopee.tw/product/9241878/13041538258', 'https://shopee.tw/product/9241878/532288390', 'https://shopee.tw/product/9241878/532316863', 'https://shopee.tw/product/9241878/9475185728']\n"
     ]
    }
   ],
   "source": [
    "curr_dir = os.getcwd()\n",
    "excel_file_path = os.path.join(curr_dir, 'Shoppee_Ratings.xlsx')\n",
    "column_name = 'url' \n",
    "df = pd.read_excel(excel_file_path)\n",
    "product_urls = df[column_name].tolist()\n",
    "product_urls = product_urls[:10]\n",
    "print(product_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_customer_name(element):\n",
    "    name=\"\"\n",
    "    try:\n",
    "        name = element.find_element(By.CLASS_NAME, 'shopee-product-rating__author-name').text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while extracting name data: {e}\")\n",
    "    return name\n",
    "\n",
    "def extract_profile_url(element):\n",
    "    url=\"\"\n",
    "    try:\n",
    "        url = element.find_element(By.CLASS_NAME, 'shopee-product-rating__author-name').get_attribute(\"href\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while extracting url data: {e}\")\n",
    "    return url\n",
    "\n",
    "def extract_rating(element):\n",
    "    rating=\"\"\n",
    "    try:\n",
    "        svg_elements = element.find_element(By.CLASS_NAME, 'shopee-product-rating__rating')\n",
    "        rating = len(svg_elements.find_elements(By.CLASS_NAME, 'icon-rating-solid--active'))\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while extracting rating data: {e}\")\n",
    "    return rating\n",
    "\n",
    "def extract_time(element):\n",
    "    time=\"\"\n",
    "    try:\n",
    "        time = element.find_element(By.CLASS_NAME, 'shopee-product-rating__time').text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while extracting time data: {e}\")\n",
    "    return time\n",
    "\n",
    "def extract_review_data(element):\n",
    "    review_data = \"\"\n",
    "    try:\n",
    "        container = element.find_element(By.CSS_SELECTOR, '.Rk6V\\\\+3')\n",
    "        div_elements = container.find_elements(By.TAG_NAME, \"div\")\n",
    "        for div in div_elements:\n",
    "            text = div.text.strip()\n",
    "            if text: \n",
    "                review_data += text + '\\n'\n",
    "    except NoSuchElementException as e:\n",
    "        return review_data \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while extracting review data: {e}\")\n",
    "    return review_data\n",
    "\n",
    "def extract_image_url(element):\n",
    "    image_urls = []\n",
    "    try:\n",
    "        parent_element = element.find_element(By.CLASS_NAME, \"rating-media-list__zoomed-image\")\n",
    "        image_elements = parent_element.find_elements(By.TAG_NAME, \"img\")\n",
    "        urls = [element.get_attribute(\"src\") for element in image_elements]\n",
    "        image_urls = urls\n",
    "    except NoSuchElementException as e:\n",
    "        return image_urls \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while extracting image URLs: {e}\")\n",
    "    return image_urls\n",
    "\n",
    "def extract_video_url(element):\n",
    "    video_urls = []\n",
    "    try:\n",
    "        parent_element = element.find_element(By.CLASS_NAME, \"rating-media-list__zoomed-image\")\n",
    "        video_elements = parent_element.find_elements(By.TAG_NAME, \"video\")\n",
    "        urls = [element.get_attribute(\"src\") for element in video_elements]\n",
    "        video_urls = urls\n",
    "    except NoSuchElementException as e:\n",
    "        return video_urls \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while extracting image URLs: {e}\")\n",
    "    return video_urls\n",
    "\n",
    "def extract_likes(element):\n",
    "    likes=\"\"\n",
    "    try:\n",
    "        likes = element.find_element(By.CLASS_NAME, 'shopee-product-rating__like-count').text\n",
    "        if likes == \"Helpful?\":\n",
    "            likes = 0\n",
    "    except NoSuchElementException as e:\n",
    "        return 0 \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while extracting image URLs: {e}\")\n",
    "    return likes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_reviews(url):\n",
    "    reviews = []\n",
    "    global timestamp_last_scrape\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        scroll_count = 5\n",
    "        for _ in range(scroll_count):\n",
    "            driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)\n",
    "            time.sleep(2)\n",
    "\n",
    "        product_name = driver.find_element(By.CLASS_NAME, \"_44qnta\").text\n",
    "\n",
    "        current_page = 1  \n",
    "        while True:\n",
    "            parent_elements=driver.find_elements(By.CLASS_NAME, \"shopee-product-rating__main\")\n",
    "            \n",
    "            if parent_elements:\n",
    "                for i in range(len(parent_elements)):\n",
    "                    customer_name = extract_customer_name(parent_elements[i])\n",
    "                    customer_profile_url = extract_profile_url(parent_elements[i])\n",
    "                    rating = extract_rating(parent_elements[i])\n",
    "                    timeStamp=extract_time(parent_elements[i])\n",
    "                    review = extract_review_data(parent_elements[i])\n",
    "                    video_urls=extract_video_url(parent_elements[i])\n",
    "                    image_urls=extract_image_url(parent_elements[i])\n",
    "                    media_urls=video_urls+image_urls\n",
    "                    likes = extract_likes(parent_elements[i])\n",
    "                    reviews.append([product_name, url, \"Historical\", customer_name, customer_profile_url, rating, timeStamp, review, media_urls, likes])\n",
    "\n",
    "            try:\n",
    "                next_page_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.CLASS_NAME, 'shopee-icon-button--right'))\n",
    "                )\n",
    "                ActionChains(driver).move_to_element(next_page_button).perform()\n",
    "                next_page_button.click()\n",
    "                time.sleep(2)\n",
    "                new_page = driver.find_element(By.CLASS_NAME, 'shopee-button-solid.shopee-button-solid--primary').text\n",
    "                if new_page == str(current_page):\n",
    "                    break\n",
    "                current_page = new_page\n",
    "            except TimeoutException:\n",
    "                print(\"Pagination button is not clickable. Exiting pagination.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while navigating to the next page: {e}\")\n",
    "                break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-11 16:52\n",
      "Directory '2187035894' created successfully\n",
      "An error occurred while navigating to the next page: Message: element click intercepted: Element <button class=\"shopee-icon-button shopee-icon-button--right \">...</button> is not clickable at point (826, 886). Other element would receive the click: <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 20 20\" class=\"chat-icon\">...</svg>\n",
      "  (Session info: chrome=117.0.5938.150)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF6F2BC7D12+55474]\n",
      "\t(No symbol) [0x00007FF6F2B377C2]\n",
      "\t(No symbol) [0x00007FF6F29EE0EB]\n",
      "\t(No symbol) [0x00007FF6F2A34E7B]\n",
      "\t(No symbol) [0x00007FF6F2A33249]\n",
      "\t(No symbol) [0x00007FF6F2A31018]\n",
      "\t(No symbol) [0x00007FF6F2A300D3]\n",
      "\t(No symbol) [0x00007FF6F2A25DCF]\n",
      "\t(No symbol) [0x00007FF6F2A4F15A]\n",
      "\t(No symbol) [0x00007FF6F2A256E6]\n",
      "\t(No symbol) [0x00007FF6F2A4F370]\n",
      "\t(No symbol) [0x00007FF6F2A67EF2]\n",
      "\t(No symbol) [0x00007FF6F2A4EF33]\n",
      "\t(No symbol) [0x00007FF6F2A23D41]\n",
      "\t(No symbol) [0x00007FF6F2A24F84]\n",
      "\tGetHandleVerifier [0x00007FF6F2F2B762+3609346]\n",
      "\tGetHandleVerifier [0x00007FF6F2F81A80+3962400]\n",
      "\tGetHandleVerifier [0x00007FF6F2F79F0F+3930799]\n",
      "\tGetHandleVerifier [0x00007FF6F2C63CA6+694342]\n",
      "\t(No symbol) [0x00007FF6F2B42218]\n",
      "\t(No symbol) [0x00007FF6F2B3E484]\n",
      "\t(No symbol) [0x00007FF6F2B3E5B2]\n",
      "\t(No symbol) [0x00007FF6F2B2EE13]\n",
      "\tBaseThreadInitThunk [0x00007FFB462F257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFB47FCAA78+40]\n",
      "\n",
      "Scraping completed for URL 'https://shopee.tw/product/9241878/21870358942'. Latest data (not in past_reviews.json) saved in 'c:\\Users\\VICTUS\\OneDrive\\Desktop\\ShoppeDataCrawler\\scraped_data\\2187035894' folder.\n",
      "Directory '1268779784' created successfully\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\VICTUS\\OneDrive\\Desktop\\ShoppeDataCrawler\\shopee_scrapping_using_selenium.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/VICTUS/OneDrive/Desktop/ShoppeDataCrawler/shopee_scrapping_using_selenium.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m data\u001b[39m=\u001b[39m[]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/VICTUS/OneDrive/Desktop/ShoppeDataCrawler/shopee_scrapping_using_selenium.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m folder_path \u001b[39m=\u001b[39m create_folder(url)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/VICTUS/OneDrive/Desktop/ShoppeDataCrawler/shopee_scrapping_using_selenium.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m reviews \u001b[39m=\u001b[39m scrape_reviews(url)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/VICTUS/OneDrive/Desktop/ShoppeDataCrawler/shopee_scrapping_using_selenium.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mif\u001b[39;00m timestamp_last_scrape  \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/VICTUS/OneDrive/Desktop/ShoppeDataCrawler/shopee_scrapping_using_selenium.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mfor\u001b[39;00m review \u001b[39min\u001b[39;00m reviews:\n",
      "\u001b[1;32mc:\\Users\\VICTUS\\OneDrive\\Desktop\\ShoppeDataCrawler\\shopee_scrapping_using_selenium.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/VICTUS/OneDrive/Desktop/ShoppeDataCrawler/shopee_scrapping_using_selenium.ipynb#W3sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m ActionChains(driver)\u001b[39m.\u001b[39mmove_to_element(next_page_button)\u001b[39m.\u001b[39mperform()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/VICTUS/OneDrive/Desktop/ShoppeDataCrawler/shopee_scrapping_using_selenium.ipynb#W3sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m next_page_button\u001b[39m.\u001b[39mclick()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/VICTUS/OneDrive/Desktop/ShoppeDataCrawler/shopee_scrapping_using_selenium.ipynb#W3sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/VICTUS/OneDrive/Desktop/ShoppeDataCrawler/shopee_scrapping_using_selenium.ipynb#W3sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m new_page \u001b[39m=\u001b[39m driver\u001b[39m.\u001b[39mfind_element(By\u001b[39m.\u001b[39mCLASS_NAME, \u001b[39m'\u001b[39m\u001b[39mshopee-button-solid.shopee-button-solid--primary\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mtext\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/VICTUS/OneDrive/Desktop/ShoppeDataCrawler/shopee_scrapping_using_selenium.ipynb#W3sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mif\u001b[39;00m new_page \u001b[39m==\u001b[39m \u001b[39mstr\u001b[39m(current_page):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def create_folder(url):\n",
    "    a = urlparse(url)\n",
    "    folder_name = os.path.basename(a.path)\n",
    "    folder_name = folder_name[:10]\n",
    "    path = os.path.join(os.getcwd(), \"scraped_data\", folder_name)\n",
    "    try: \n",
    "        os.makedirs(path, exist_ok=True) \n",
    "        print(\"Directory '%s' created successfully\" % folder_name) \n",
    "    except OSError as error: \n",
    "        print(\"Directory '%s' can not be created\" % folder_name) \n",
    "    return path\n",
    "\n",
    "print(timestamp_last_scrape)\n",
    "# timestamp_last_scrape = \"2023-10-1 18:41\"\n",
    "# timestamp_last_scrape = None\n",
    "\n",
    "\n",
    "for url in product_urls:\n",
    "    data=[]\n",
    "    folder_path = create_folder(url)\n",
    "    reviews = scrape_reviews(url)\n",
    "\n",
    "    if timestamp_last_scrape  is None:\n",
    "        for review in reviews:\n",
    "            data.append({\n",
    "                \"Radarly pid\": 6127,\n",
    "                \"Radarly corpusId\": 64143,\n",
    "                \"Radarly corpusName\": \"Feminine Care\",\n",
    "                \"Product_Name\": review[0],\n",
    "                \"Product_url\": review[1],\n",
    "                \"Status\": review[2],\n",
    "                \"Name\": review[3],\n",
    "                \"Customer_Profile_Url\": review[4],\n",
    "                \"Rating\": review[5],\n",
    "                \"TimeStamps\": review[6],\n",
    "                \"Review\": review[7],\n",
    "                \"Media_Url\": review[8],\n",
    "                \"Likes\": review[9],\n",
    "                \"Id\": \"\"\n",
    "            })\n",
    "\n",
    "        with open(os.path.join(folder_path, \"past_reviews.json\"), 'w', encoding='utf-8') as past_json_file:\n",
    "            json.dump(data, past_json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "        with open(os.path.join(folder_path, \"latest_reviews.json\"), 'w', encoding='utf-8') as latest_json_file:\n",
    "            json.dump(data, latest_json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Scraping completed for URL '{url}'. Data saved in '{folder_path}' folder.\")\n",
    "    else:\n",
    "        temp_timestamp_last_scrape = dt.datetime.strptime(timestamp_last_scrape, '%Y-%m-%d %H:%M')\n",
    "\n",
    "        with open(os.path.join(folder_path, \"past_reviews.json\"), 'r', encoding='utf-8') as past_json_file:\n",
    "            past_reviews_data = json.load(past_json_file)\n",
    "            \n",
    "        for review in reviews:\n",
    "            full_timestamp = review[6]\n",
    "            timestamp_match = re.search(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}', full_timestamp)\n",
    "            if timestamp_match:\n",
    "                timestamp_part = timestamp_match.group(0)\n",
    "            else:\n",
    "                continue\n",
    "                    \n",
    "            try:    \n",
    "                timestamp_part = dt.datetime.strptime(timestamp_part, '%Y-%m-%d %H:%M')\n",
    "                if timestamp_part > temp_timestamp_last_scrape:\n",
    "                    data.append({\n",
    "                        \"Radarly pid\": 6127,\n",
    "                        \"Radarly corpusId\": 64143,\n",
    "                        \"Radarly corpusName\": \"Feminine Care\",\n",
    "                        \"Product_Name\": review[0],\n",
    "                        \"Product_url\": review[1],\n",
    "                        \"Status\": review[2],\n",
    "                        \"Name\": review[3],\n",
    "                        \"Customer_Profile_Url\": review[4],\n",
    "                        \"Rating\": review[5],\n",
    "                        \"TimeStamps\": timestamp_part.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "                        \"Review\": review[7],\n",
    "                        \"Media_Url\": review[8],\n",
    "                        \"Likes\": review[9],\n",
    "                        \"Id\": \"\"\n",
    "                    })\n",
    "            except ValueError:\n",
    "                print(f\"Error parsing timestamp: {timestamp_part}\")\n",
    "                continue\n",
    "        \n",
    "        with open(os.path.join(folder_path, \"latest_reviews.json\"), 'w', encoding='utf-8') as latest_json_file:\n",
    "            json.dump(data, latest_json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Scraping completed for URL '{url}'. Latest data (not in past_reviews.json) saved in '{folder_path}' folder.\")\n",
    "\n",
    "current_datetime = dt.datetime.now()\n",
    "timestamp_last_scrape = current_datetime.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store in csv file\n",
    "\n",
    "with open(csv_filename, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow([\"Radarly pid\", \"Radarly corpusId\", \"Radarly corpusName\", \"Product_Name\", \"Product_url\",  \"Status\", \"Name\", \"Customer_Profile_Url\", \"Rating\", \"TimeStamps\", \"Review\", \"Media_Url\", \"Likes\", \"Id\"])\n",
    "    \n",
    "    for url in product_urls:\n",
    "        historical_reviews = scrape_reviews(url)\n",
    "\n",
    "        for review in historical_reviews:\n",
    "            csv_writer.writerow([6127, 64143, \"Feminine Care\", review[0], review[1], review[2], review[3], review[4], review[5], review[6], review[7], review[8], review[9], \"\"])\n",
    "\n",
    "print(f\"Scraping completed. Data saved in '{csv_filename}'.\")\n",
    "\n",
    "df=pd.read_csv(\"store.csv\")\n",
    "df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
