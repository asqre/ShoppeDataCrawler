{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# Function to scrape historical reviews\n",
    "def scrape_historical_reviews(url):\n",
    "    reviews = []\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        product_name_element = soup.find('div', class_='product-page-title')\n",
    "        if product_name_element:\n",
    "            product_name = product_name_element.text.strip()\n",
    "\n",
    "            review_elements = soup.find_all('div', class_='product-ratings')\n",
    "            for element in review_elements:\n",
    "                rating = element.find('div', class_='product-rating').text.strip()\n",
    "                review_text = element.find('div', class_='shopee-product-rating__content').text.strip()\n",
    "                reviews.append([product_name, url, \"Historical\", rating, review_text])\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# Function to scrape day-to-day new reviews\n",
    "def scrape_new_reviews(url):\n",
    "    reviews = []\n",
    "    try:\n",
    "        response = requests.get(url + '/rating')\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        product_name_element = soup.find('div', class_='product-page-title')\n",
    "        if product_name_element:\n",
    "            product_name = product_name_element.text.strip()\n",
    "\n",
    "            review_elements = soup.find_all('div', class_='shopee-product-comment')\n",
    "            for element in review_elements:\n",
    "                review_text = element.find('div', class_='shopee-product-comment__content').text.strip()\n",
    "                reviews.append([product_name, url, \"New\", \"\", review_text])\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return reviews\n",
    "\n",
    "product_urls = [\n",
    "    \"https://shopee.tw/%E3%80%90%E9%80%A2%E7%94%B2FUZZY%E3%80%91Nike-Dunk-Low-%E9%BB%91%E7%99%BD-%E7%86%8A%E8%B2%93-DD1391-100-DD1503-101-CW1590-i.6783271.6874570040?sp_atk=a11997b1-2f60-484c-b2d1-bd2914d2bc9f&xptdk=a11997b1-2f60-484c-b2d1-bd2914d2bc9f/\",\n",
    "    # \"https://shopee.tw/product/9241878/12687797847\",\n",
    "]\n",
    "\n",
    "csv_filename = \"shopee_reviews.csv\"\n",
    "\n",
    "with open(csv_filename, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow([\"Radarly pid\", \"Radarly corpusId\", \"Radarly corpusName\", \"name\", \"url\", \"Id\"])\n",
    "\n",
    "    for url in product_urls:\n",
    "        historical_reviews = scrape_historical_reviews(url)\n",
    "        new_reviews = scrape_new_reviews(url)\n",
    "\n",
    "        for review in historical_reviews:\n",
    "            csv_writer.writerow([6127, 64143, \"Feminine Care\", review[0], review[1], \"\"])\n",
    "\n",
    "        for review in new_reviews:\n",
    "            csv_writer.writerow([6127, 64143, \"Feminine Care\", review[0], review[1], \"\"])\n",
    "\n",
    "print(f\"Scraping completed. Data saved in '{csv_filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"shopee_reviews.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... (previous code)\n",
    "\n",
    "data = []\n",
    "timestamp_last_scrape = \"2023-10-01 16:29\"\n",
    "new_reviews = []  # Initialize a list to store the latest reviews\n",
    "\n",
    "if timestamp_last_scrape is None:\n",
    "    for url in product_urls:\n",
    "        historical_reviews = scrape_historical_reviews(url)\n",
    "\n",
    "        for review in historical_reviews:\n",
    "            data.append({\n",
    "                \"Radarly pid\": 6127,\n",
    "                \"Radarly corpusId\": 64143,\n",
    "                \"Radarly corpusName\": \"Feminine Care\",\n",
    "                \"Product_Name\": review[0],\n",
    "                \"Product_url\": review[1],\n",
    "                \"Status\": review[2],\n",
    "                \"Name\": review[3],\n",
    "                \"Customer_Profile_Url\": review[4],\n",
    "                \"Rating\": review[5],\n",
    "                \"TimeStamps\": review[6],\n",
    "                \"Review\": review[7],\n",
    "                \"Media_Url\": review[8],\n",
    "                \"Likes\": review[9],\n",
    "                \"Id\": \"\"\n",
    "            })\n",
    "\n",
    "    with open(\"past_reviews.json\", 'w', encoding='utf-8') as past_json_file:\n",
    "        json.dump(data, past_json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    with open(\"latest_reviews.json\", 'w', encoding='utf-8') as latest_json_file:\n",
    "        json.dump(data, latest_json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    current_datetime = dt.datetime.now()\n",
    "    timestamp_last_scrape = current_datetime.strftime(\"%Y-%m-%d %H:%M\")\n",
    "    \n",
    "    print(f\"Scraping completed. Data saved in 'past_reviews.json' and 'latest_reviews.json'.\")\n",
    "else:\n",
    "    with open(\"past_reviews.json\", 'r', encoding='utf-8') as past_json_file:\n",
    "        past_reviews_data = json.load(past_json_file)\n",
    "\n",
    "    for url in product_urls:\n",
    "        latest_reviews = scrape_historical_reviews(url)\n",
    "        for review in latest_reviews:\n",
    "            \n",
    "            full_timestamp = review[6]\n",
    "            timestamp_match = re.search(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}', full_timestamp)\n",
    "            if timestamp_match:\n",
    "                timestamp_part = timestamp_match.group(0)\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            try:    \n",
    "                timestamp_part = dt.datetime.strptime(timestamp_part, '%Y-%m-%d %H:%M')\n",
    "\n",
    "                if timestamp_part > timestamp_last_scrape:\n",
    "                    new_reviews.append({\n",
    "                        \"Radarly pid\": 6127,\n",
    "                        \"Radarly corpusId\": 64143,\n",
    "                        \"Radarly corpusName\": \"Feminine Care\",\n",
    "                        \"Product_Name\": review[0],\n",
    "                        \"Product_url\": review[1],\n",
    "                        \"Status\": review[2],\n",
    "                        \"Name\": review[3],\n",
    "                        \"Customer_Profile_Url\": review[4],\n",
    "                        \"Rating\": review[5],\n",
    "                        \"TimeStamps\": timestamp_part.strftime(\"%Y-%m-%d %H:%M\"),  # Format it as a string\n",
    "                        \"Review\": review[7],\n",
    "                        \"Media_Url\": review[8],\n",
    "                        \"Likes\": review[9],\n",
    "                        \"Id\": \"\"\n",
    "                    })\n",
    "            except ValueError:\n",
    "                print(f\"Error parsing timestamp: {timestamp_part}\")\n",
    "                continue\n",
    "    \n",
    "    with open(\"latest_reviews.json\", 'w', encoding='utf-8') as latest_json_file:\n",
    "        json.dump(new_reviews, latest_json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Scraping completed. Latest data (not in past_reviews.json) saved in 'latest_reviews.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Import the os module to manage directories\n",
    "\n",
    "# ... (previous code)\n",
    "\n",
    "# Define a function to create a folder for a specific URL\n",
    "def create_folder(url):\n",
    "    folder_name = re.sub(r'[^\\w\\s-]', '', url)  # Remove special characters from URL to create a folder name\n",
    "    folder_name = folder_name.replace(' ', '_')  # Replace spaces with underscores\n",
    "    folder_path = os.path.join(os.getcwd(), folder_name)  # Create a folder path based on the current directory\n",
    "    os.makedirs(folder_path, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "    return folder_path\n",
    "\n",
    "data = []\n",
    "timestamp_last_scrape = \"2023-10-1 16:29\"\n",
    "\n",
    "if timestamp_last_scrape is not None:\n",
    "    timestamp_last_scrape = dt.datetime.strptime(timestamp_last_scrape, '%Y-%m-%d %H:%M')\n",
    "\n",
    "for url in product_urls:\n",
    "    folder_path = create_folder(url)  # Create a folder for each URL\n",
    "\n",
    "    if timestamp_last_scrape is None:\n",
    "        historical_reviews = scrape_historical_reviews(url)\n",
    "\n",
    "        for review in historical_reviews:\n",
    "            data.append({\n",
    "                \"Radarly pid\": 6127,\n",
    "                \"Radarly corpusId\": 64143,\n",
    "                \"Radarly corpusName\": \"Feminine Care\",\n",
    "                \"Product_Name\": review[0],\n",
    "                \"Product_url\": review[1],\n",
    "                \"Status\": review[2],\n",
    "                \"Name\": review[3],\n",
    "                \"Customer_Profile_Url\": review[4],\n",
    "                \"Rating\": review[5],\n",
    "                \"TimeStamps\": review[6],\n",
    "                \"Review\": review[7],\n",
    "                \"Media_Url\": review[8],\n",
    "                \"Likes\": review[9],\n",
    "                \"Id\": \"\"\n",
    "            })\n",
    "\n",
    "        # Save the data in the folder\n",
    "        with open(os.path.join(folder_path, \"past_reviews.json\"), 'w', encoding='utf-8') as past_json_file:\n",
    "            json.dump(data, past_json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "        with open(os.path.join(folder_path, \"latest_reviews.json\"), 'w', encoding='utf-8') as latest_json_file:\n",
    "            json.dump(data, latest_json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "        current_datetime = dt.datetime.now()\n",
    "        timestamp_last_scrape = current_datetime.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "        print(f\"Scraping completed for URL '{url}'. Data saved in '{folder_path}' folder.\")\n",
    "    else:\n",
    "        with open(os.path.join(folder_path, \"past_reviews.json\"), 'r', encoding='utf-8') as past_json_file:\n",
    "            past_reviews_data = json.load(past_json_file)\n",
    "\n",
    "        for review in latest_reviews:\n",
    "            full_timestamp = review[6]\n",
    "            timestamp_match = re.search(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}', full_timestamp)\n",
    "            if timestamp_match:\n",
    "                timestamp_part = timestamp_match.group(0)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                timestamp_part = dt.datetime.strptime(timestamp_part, '%Y-%m-%d %H:%M')\n",
    "                if timestamp_part > timestamp_last_scrape:\n",
    "                    data.append({\n",
    "                        \"Radarly pid\": 6127,\n",
    "                        \"Radarly corpusId\": 64143,\n",
    "                        \"Radarly corpusName\": \"Feminine Care\",\n",
    "                        \"Product_Name\": review[0],\n",
    "                        \"Product_url\": review[1],\n",
    "                        \"Status\": review[2],\n",
    "                        \"Name\": review[3],\n",
    "                        \"Customer_Profile_Url\": review[4],\n",
    "                        \"Rating\": review[5],\n",
    "                        \"TimeStamps\": timestamp_part.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "                        \"Review\": review[7],\n",
    "                        \"Media_Url\": review[8],\n",
    "                        \"Likes\": review[9],\n",
    "                        \"Id\": \"\"\n",
    "                    })\n",
    "            except ValueError:\n",
    "                print(f\"Error parsing timestamp: {timestamp_part}\")\n",
    "                continue\n",
    "\n",
    "        # Save the latest data in the folder\n",
    "        with open(os.path.join(folder_path, \"latest_reviews.json\"), 'w', encoding='utf-8') as latest_json_file:\n",
    "            json.dump(data, latest_json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Scraping completed for URL '{url}'. Latest data (not in past_reviews.json) saved in '{folder_path}' folder.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
